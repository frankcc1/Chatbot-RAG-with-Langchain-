{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80fe3d3c",
   "metadata": {},
   "source": [
    "PIPELINE RAG - DATA INGESTION TO VECTOR DB PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ae81ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\Practica\\Proyectos\\Chatbot-RAG-with-Langchain-\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader,TextLoader, CSVLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders.excel import UnstructuredExcelLoader\n",
    "from pathlib import Path\n",
    "from typing import List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97fb2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion que lee y transforma data en estructura document langchain\n",
    "def load_all_documents(data_dir: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Load all supported files from the data directory and convert to LangChain document structure.\n",
    "    Supported: PDF, TXT, CSV, Excel, Word, JSON\n",
    "    \"\"\"\n",
    "    # Use project root data folder\n",
    "    data_path = Path(data_dir).resolve()\n",
    "    print(f\"[DEBUG] Data path: {data_path}\")\n",
    "    documents = []\n",
    "\n",
    "    # PDF files\n",
    "    pdf_files = list(data_path.glob('**/*.pdf'))\n",
    "    print(f\"[DEBUG] Found {len(pdf_files)} PDF files: {[str(f) for f in pdf_files]}\")\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"[DEBUG] Loading PDF: {pdf_file}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} PDF docs from {pdf_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load PDF {pdf_file}: {e}\")\n",
    "\n",
    "    # TXT files\n",
    "    txt_files = list(data_path.glob('**/*.txt'))\n",
    "    print(f\"[DEBUG] Found {len(txt_files)} TXT files: {[str(f) for f in txt_files]}\")\n",
    "    for txt_file in txt_files:\n",
    "        print(f\"[DEBUG] Loading TXT: {txt_file}\")\n",
    "        try:\n",
    "            loader = TextLoader(str(txt_file) , encoding=\"utf-8\")\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} TXT docs from {txt_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load TXT {txt_file}: {e}\")\n",
    "\n",
    "    # CSV files\n",
    "    csv_files = list(data_path.glob('**/*.csv'))\n",
    "    print(f\"[DEBUG] Found {len(csv_files)} CSV files: {[str(f) for f in csv_files]}\")\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"[DEBUG] Loading CSV: {csv_file}\")\n",
    "        try:\n",
    "            loader = CSVLoader(str(csv_file))\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} CSV docs from {csv_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load CSV {csv_file}: {e}\")\n",
    "\n",
    "    # Excel files\n",
    "    xlsx_files = list(data_path.glob('**/*.xlsx'))\n",
    "    print(f\"[DEBUG] Found {len(xlsx_files)} Excel files: {[str(f) for f in xlsx_files]}\")\n",
    "    for xlsx_file in xlsx_files:\n",
    "        print(f\"[DEBUG] Loading Excel: {xlsx_file}\")\n",
    "        try:\n",
    "            loader = UnstructuredExcelLoader(str(xlsx_file))\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} Excel docs from {xlsx_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load Excel {xlsx_file}: {e}\")\n",
    "\n",
    "    # Word files\n",
    "    docx_files = list(data_path.glob('**/*.docx'))\n",
    "    print(f\"[DEBUG] Found {len(docx_files)} Word files: {[str(f) for f in docx_files]}\")\n",
    "    for docx_file in docx_files:\n",
    "        print(f\"[DEBUG] Loading Word: {docx_file}\")\n",
    "        try:\n",
    "            loader = Docx2txtLoader(str(docx_file))\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} Word docs from {docx_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load Word {docx_file}: {e}\")\n",
    "\n",
    "    # JSON files\n",
    "    json_files = list(data_path.glob('**/*.json'))\n",
    "    print(f\"[DEBUG] Found {len(json_files)} JSON files: {[str(f) for f in json_files]}\")\n",
    "    for json_file in json_files:\n",
    "        print(f\"[DEBUG] Loading JSON: {json_file}\")\n",
    "        try:\n",
    "            loader = JSONLoader(str(json_file))\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} JSON docs from {json_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load JSON {json_file}: {e}\")\n",
    "\n",
    "    print(f\"[DEBUG] Total loaded documents: {len(documents)}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae097bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split LangChain documents into smaller chunks for RAG.\n",
    "    \"\"\" \n",
    "    print(f\"[INFO] Splitting {len(documents)} documents...\")\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"[INFO] Generated {len(chunks)} chunks from {len(documents)} documents.\")\n",
    "\n",
    "    # Ejemplo de chunk (solo para aprender)\n",
    "    if chunks:\n",
    "        print(\"\\n[DEBUG] Example chunk:\")\n",
    "        print(\"Content:\", chunks[0].page_content[:200], \"...\")\n",
    "        print(\"Metadata:\", chunks[0].metadata)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb03252",
   "metadata": {},
   "source": [
    "Embedding and VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a73d97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c32c32e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x22f6a206660>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Generar embeddings usando SentenceTransformer\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "\n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generar embeddings para una lista de textos\n",
    "\n",
    "        Args:\n",
    "            texts: Lista de cadenas de texto para convertir a embeddings\n",
    "\n",
    "        Returns:\n",
    "            Arreglo numpy de embeddings con la forma (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "    \n",
    "        if not self.model:\n",
    "            raise ValueError(\"El modelo no está cargado\")\n",
    "\n",
    "        print(f\"Generando embeddings para {len(texts)} textos...\")\n",
    "        \n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "        print(f\"Embeddings generados con forma: {embeddings.shape}\")\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "#Inicializando el embeding\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe25e67",
   "metadata": {},
   "source": [
    "VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e19c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Colección 'base_conocimiento' cargada correctamente en ../data/vector_store\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x22f7c6b8ad0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Gestiona los embeddings de documentos en una base de datos vectorial ChromaDB\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str = \"base_conocimiento\", persist_directory: str = \"../data/vector_store\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"\n",
    "        Inicializa el cliente de ChromaDB y crea/carga la colección.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Usamos PersistentClient para guardar datos en disco\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            # Crea la colexion en caso no exista, si existe la usa. (Coleccion = Una caja donde van los textos + embeddings)\n",
    "            self.collection = self.client.get_or_create_collection(name=self.collection_name)\n",
    "            print(f\"[INFO] Colección '{self.collection_name}' cargada correctamente en {self.persist_directory}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Fallo al inicializar ChromaDB: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, chunks: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Añade documentos (chunks) y sus embeddings a la base de datos.\n",
    "        \"\"\"\n",
    "        if not chunks or len(chunks) == 0:\n",
    "            print(\"[WARN] No hay documentos para agregar.\")\n",
    "            return\n",
    "\n",
    "        print(f\"[INFO] Añadiendo {len(chunks)} documentos a ChromaDB...\")\n",
    "        \n",
    "        # Prepara los datos para Chroma\n",
    "        ids = [str(uuid.uuid4()) for _ in range(len(chunks))] # IDs únicos para cada chunk\n",
    "        documents = [chunk.page_content for chunk in chunks]  # Extraccion de texto de cada chunk\n",
    "        metadatas = [chunk.metadata for chunk in chunks]      # Extraccion de metadatos (origen, página, etc.)\n",
    "        \n",
    "        # Chroma espera listas, y los embeddings deben ser listas de python, no numpy arrays a veces\n",
    "        embeddings_list = embeddings.tolist() if isinstance(embeddings, np.ndarray) else embeddings\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                documents=documents,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "            print(f\"[INFO] Éxito: {len(chunks)} chunks guardados.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Fallo al agregar documentos: {e}\")\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fd80f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(self, query_embedding: np.ndarray, k: int = 5):\n",
    "        \"\"\"\n",
    "        Busca los documentos más similares al embedding de la consulta.\n",
    "        5 resultados\n",
    "        \"\"\"\n",
    "        print(f\"[INFO] Buscando los {k} fragmentos más relevantes...\")\n",
    "        try:\n",
    "            # Convertir a lista (la pregunta del usuario) si es numpy\n",
    "            query_list = query_embedding.tolist() if isinstance(query_embedding, np.ndarray) else query_embedding\n",
    "            # Busca en ChromaDB los textos más parecidos a traves de los embedings\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=query_list,\n",
    "                n_results=k\n",
    "            )\n",
    "            # Devuelve los 5 resultados más parecidos\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Fallo en la búsqueda: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5df95dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Data path: C:\\Users\\HP\\Desktop\\Practica\\Proyectos\\Chatbot-RAG-with-Langchain-\\data\n",
      "[DEBUG] Found 2 PDF files: ['C:\\\\Users\\\\HP\\\\Desktop\\\\Practica\\\\Proyectos\\\\Chatbot-RAG-with-Langchain-\\\\data\\\\pdf\\\\Cv_Derecho.pdf', 'C:\\\\Users\\\\HP\\\\Desktop\\\\Practica\\\\Proyectos\\\\Chatbot-RAG-with-Langchain-\\\\data\\\\pdf\\\\Frank Casana Casimiro CV.pdf']\n",
      "[DEBUG] Loading PDF: C:\\Users\\HP\\Desktop\\Practica\\Proyectos\\Chatbot-RAG-with-Langchain-\\data\\pdf\\Cv_Derecho.pdf\n",
      "[DEBUG] Loaded 2 PDF docs from C:\\Users\\HP\\Desktop\\Practica\\Proyectos\\Chatbot-RAG-with-Langchain-\\data\\pdf\\Cv_Derecho.pdf\n",
      "[DEBUG] Loading PDF: C:\\Users\\HP\\Desktop\\Practica\\Proyectos\\Chatbot-RAG-with-Langchain-\\data\\pdf\\Frank Casana Casimiro CV.pdf\n",
      "[DEBUG] Loaded 2 PDF docs from C:\\Users\\HP\\Desktop\\Practica\\Proyectos\\Chatbot-RAG-with-Langchain-\\data\\pdf\\Frank Casana Casimiro CV.pdf\n",
      "[DEBUG] Found 1 TXT files: ['C:\\\\Users\\\\HP\\\\Desktop\\\\Practica\\\\Proyectos\\\\Chatbot-RAG-with-Langchain-\\\\data\\\\text_files\\\\IA.txt']\n",
      "[DEBUG] Loading TXT: C:\\Users\\HP\\Desktop\\Practica\\Proyectos\\Chatbot-RAG-with-Langchain-\\data\\text_files\\IA.txt\n",
      "[DEBUG] Loaded 1 TXT docs from C:\\Users\\HP\\Desktop\\Practica\\Proyectos\\Chatbot-RAG-with-Langchain-\\data\\text_files\\IA.txt\n",
      "[DEBUG] Found 0 CSV files: []\n",
      "[DEBUG] Found 0 Excel files: []\n",
      "[DEBUG] Found 0 Word files: []\n",
      "[DEBUG] Found 0 JSON files: []\n",
      "[DEBUG] Total loaded documents: 5\n",
      "[INFO] Splitting 5 documents...\n",
      "[INFO] Generated 14 chunks from 5 documents.\n",
      "\n",
      "[DEBUG] Example chunk:\n",
      "Content: Thalia Gilio \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " 989718466 • thalia.v.gilio21@gmail.com  •linkedin.com/in/thaliagilio21 • Lima, Perú  \n",
      "Presentación \n",
      " \n",
      "Soy Bachiller de la carrera de Derecho con mucho interés en profundizar con ...\n",
      "Metadata: {'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-10-26T21:55:13-05:00', 'author': 'Gregg Rosenblum', 'moddate': '2025-10-26T21:55:13-05:00', 'source': 'C:\\\\Users\\\\HP\\\\Desktop\\\\Practica\\\\Proyectos\\\\Chatbot-RAG-with-Langchain-\\\\data\\\\pdf\\\\Cv_Derecho.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}\n",
      "Generando embeddings para 14 textos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generados con forma: (14, 384)\n",
      "[INFO] Añadiendo 14 documentos a ChromaDB...\n",
      "[INFO] Éxito: 14 chunks guardados.\n"
     ]
    }
   ],
   "source": [
    "docs = load_all_documents(\"../data\")\n",
    "chunks = split_documents(docs)\n",
    "texts = [doc.page_content for doc in chunks ]\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4361ad",
   "metadata": {},
   "source": [
    "Retriever pipeline from VectorStore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c15aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Recupera informacion de la base vectorial deacuerdo a consultas\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vector_store: Base vectorial, contiene la informacion\n",
    "            embedding_manager: Convierte la consulta en vectores gracias al embedding\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store \n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Recupera informacion relevante deacuerdo a la pregunta\n",
    "        \n",
    "        Args:\n",
    "            query: Consulta del usuario\n",
    "            top_k: Numero de resultados que devolverá\n",
    "            score_threshold: Filtro de similitud, 0 (Acepta todo),0.7(muy parecidos),0.9(Estrictamente parecidos): 0-1\n",
    "            \n",
    "        Returns:\n",
    "           Lista de diccionarios de informacion y metadata.\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # La consulta del usuario transformandola a embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Busqueda en la base de datos vectorial\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Procesar resultados, en esta lista se guardaron los resultados filtrados\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                # Zip = une elementos con el mismo indice \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Conversion de distancias a similitud (ChromaDB usa distancias)\n",
    "                    similarity_score = 1 - distance\n",
    "                    # Filtro de Calidad, en \"retrieved_docs\" se guarda la data importante\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
