{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80fe3d3c",
   "metadata": {},
   "source": [
    "PIPELINE RAG - DATA INGESTION TO VECTOR DB PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ae81ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\Practica\\Proyectos\\Chatbot-RAG-with-Langchain-\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader,TextLoader, CSVLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders.excel import UnstructuredExcelLoader\n",
    "from pathlib import Path\n",
    "from typing import List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97fb2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion que lee y transforma data en estructura document langchain\n",
    "def load_all_documents(data_dir: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Load all supported files from the data directory and convert to LangChain document structure.\n",
    "    Supported: PDF, TXT, CSV, Excel, Word, JSON\n",
    "    \"\"\"\n",
    "    # Use project root data folder\n",
    "    data_path = Path(data_dir).resolve()\n",
    "    print(f\"[DEBUG] Data path: {data_path}\")\n",
    "    documents = []\n",
    "\n",
    "    # PDF files\n",
    "    pdf_files = list(data_path.glob('**/*.pdf'))\n",
    "    print(f\"[DEBUG] Found {len(pdf_files)} PDF files: {[str(f) for f in pdf_files]}\")\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"[DEBUG] Loading PDF: {pdf_file}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} PDF docs from {pdf_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load PDF {pdf_file}: {e}\")\n",
    "\n",
    "    # TXT files\n",
    "    txt_files = list(data_path.glob('**/*.txt'))\n",
    "    print(f\"[DEBUG] Found {len(txt_files)} TXT files: {[str(f) for f in txt_files]}\")\n",
    "    for txt_file in txt_files:\n",
    "        print(f\"[DEBUG] Loading TXT: {txt_file}\")\n",
    "        try:\n",
    "            loader = TextLoader(str(txt_file) , encoding=\"utf-8\")\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} TXT docs from {txt_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load TXT {txt_file}: {e}\")\n",
    "\n",
    "    # CSV files\n",
    "    csv_files = list(data_path.glob('**/*.csv'))\n",
    "    print(f\"[DEBUG] Found {len(csv_files)} CSV files: {[str(f) for f in csv_files]}\")\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"[DEBUG] Loading CSV: {csv_file}\")\n",
    "        try:\n",
    "            loader = CSVLoader(str(csv_file))\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} CSV docs from {csv_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load CSV {csv_file}: {e}\")\n",
    "\n",
    "    # Excel files\n",
    "    xlsx_files = list(data_path.glob('**/*.xlsx'))\n",
    "    print(f\"[DEBUG] Found {len(xlsx_files)} Excel files: {[str(f) for f in xlsx_files]}\")\n",
    "    for xlsx_file in xlsx_files:\n",
    "        print(f\"[DEBUG] Loading Excel: {xlsx_file}\")\n",
    "        try:\n",
    "            loader = UnstructuredExcelLoader(str(xlsx_file))\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} Excel docs from {xlsx_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load Excel {xlsx_file}: {e}\")\n",
    "\n",
    "    # Word files\n",
    "    docx_files = list(data_path.glob('**/*.docx'))\n",
    "    print(f\"[DEBUG] Found {len(docx_files)} Word files: {[str(f) for f in docx_files]}\")\n",
    "    for docx_file in docx_files:\n",
    "        print(f\"[DEBUG] Loading Word: {docx_file}\")\n",
    "        try:\n",
    "            loader = Docx2txtLoader(str(docx_file))\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} Word docs from {docx_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load Word {docx_file}: {e}\")\n",
    "\n",
    "    # JSON files\n",
    "    json_files = list(data_path.glob('**/*.json'))\n",
    "    print(f\"[DEBUG] Found {len(json_files)} JSON files: {[str(f) for f in json_files]}\")\n",
    "    for json_file in json_files:\n",
    "        print(f\"[DEBUG] Loading JSON: {json_file}\")\n",
    "        try:\n",
    "            loader = JSONLoader(str(json_file))\n",
    "            loaded = loader.load()\n",
    "            print(f\"[DEBUG] Loaded {len(loaded)} JSON docs from {json_file}\")\n",
    "            documents.extend(loaded)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load JSON {json_file}: {e}\")\n",
    "\n",
    "    print(f\"[DEBUG] Total loaded documents: {len(documents)}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae097bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split LangChain documents into smaller chunks for RAG.\n",
    "    \"\"\" \n",
    "    print(f\"[INFO] Splitting {len(documents)} documents...\")\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"[INFO] Generated {len(chunks)} chunks from {len(documents)} documents.\")\n",
    "\n",
    "    # Ejemplo de chunk (solo para aprender)\n",
    "    if chunks:\n",
    "        print(\"\\n[DEBUG] Example chunk:\")\n",
    "        print(\"Content:\", chunks[0].page_content[:200], \"...\")\n",
    "        print(\"Metadata:\", chunks[0].metadata)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb03252",
   "metadata": {},
   "source": [
    "Embedding and VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a73d97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c32c32e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x22f6a206660>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Generar embeddings usando SentenceTransformer\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "\n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generar embeddings para una lista de textos\n",
    "\n",
    "        Args:\n",
    "            texts: Lista de cadenas de texto para convertir a embeddings\n",
    "\n",
    "        Returns:\n",
    "            Arreglo numpy de embeddings con la forma (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "    \n",
    "        if not self.model:\n",
    "            raise ValueError(\"El modelo no está cargado\")\n",
    "\n",
    "        print(f\"Generando embeddings para {len(texts)} textos...\")\n",
    "        \n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "        print(f\"Embeddings generados con forma: {embeddings.shape}\")\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "#Inicializando el embeding\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe25e67",
   "metadata": {},
   "source": [
    "VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e19c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Colección 'base_conocimiento' cargada correctamente en ../data/vector_store\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x22f7c6b8ad0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Gestiona los embeddings de documentos en una base de datos vectorial ChromaDB\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str = \"base_conocimiento\", persist_directory: str = \"../data/vector_store\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"\n",
    "        Inicializa el cliente de ChromaDB y crea/carga la colección.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Usamos PersistentClient para guardar datos en disco\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            # Crea la colexion en caso no exista, si existe la usa. (Coleccion = Una caja donde van los textos + embeddings)\n",
    "            self.collection = self.client.get_or_create_collection(name=self.collection_name)\n",
    "            print(f\"[INFO] Colección '{self.collection_name}' cargada correctamente en {self.persist_directory}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Fallo al inicializar ChromaDB: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, chunks: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Añade documentos (chunks) y sus embeddings a la base de datos.\n",
    "        \"\"\"\n",
    "        if not chunks or len(chunks) == 0:\n",
    "            print(\"[WARN] No hay documentos para agregar.\")\n",
    "            return\n",
    "\n",
    "        print(f\"[INFO] Añadiendo {len(chunks)} documentos a ChromaDB...\")\n",
    "        \n",
    "        # Prepara los datos para Chroma\n",
    "        ids = [str(uuid.uuid4()) for _ in range(len(chunks))] # IDs únicos para cada chunk\n",
    "        documents = [chunk.page_content for chunk in chunks]  # Extraccion de texto de cada chunk\n",
    "        metadatas = [chunk.metadata for chunk in chunks]      # Extraccion de metadatos (origen, página, etc.)\n",
    "        \n",
    "        # Chroma espera listas, y los embeddings deben ser listas de python, no numpy arrays a veces\n",
    "        embeddings_list = embeddings.tolist() if isinstance(embeddings, np.ndarray) else embeddings\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                documents=documents,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "            print(f\"[INFO] Éxito: {len(chunks)} chunks guardados.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Fallo al agregar documentos: {e}\")\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fd80f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(self, query_embedding: np.ndarray, k: int = 5):\n",
    "        \"\"\"\n",
    "        Busca los documentos más similares al embedding de la consulta.\n",
    "        5 resultados\n",
    "        \"\"\"\n",
    "        print(f\"[INFO] Buscando los {k} fragmentos más relevantes...\")\n",
    "        try:\n",
    "            # Convertir a lista (la pregunta del usuario) si es numpy\n",
    "            query_list = query_embedding.tolist() if isinstance(query_embedding, np.ndarray) else query_embedding\n",
    "            # Busca en ChromaDB los textos más parecidos a traves de los embedings\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=query_list,\n",
    "                n_results=k\n",
    "            )\n",
    "            # Devuelve los 5 resultados más parecidos\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Fallo en la búsqueda: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df95dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_all_documents(\"../data\")\n",
    "chunks = split_documents(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
